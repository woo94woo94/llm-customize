import { ChatOpenAI } from "@langchain/openai";
import { StateGraph, MessagesAnnotation } from "@langchain/langgraph";
import { HumanMessage, AIMessage } from "@langchain/core/messages";
import { loadConfig } from "../config/index.js";

async function main() {
  // í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
  const config = loadConfig();

  console.log("=== LangGraph + ChatOpenAI ì˜ˆì œ ===\n");

  // apiUrlì—ì„œ baseURL ì¶”ì¶œ (https://api.openai.com/v1/chat/completions -> https://api.openai.com/v1)
  const baseURL = config.apiUrl.replace(/\/chat\/completions$/, "");

  // ChatOpenAI ëª¨ë¸ ìƒì„±
  const model = new ChatOpenAI({
    apiKey: config.apiKey,
    model: "gpt-4o",
    temperature: 0.7,
    configuration: {
      baseURL,
    },
  });

  // LangGraph StateGraph ì •ì˜
  const workflow = new StateGraph(MessagesAnnotation)
    .addNode("agent", async (state) => {
      // LLM í˜¸ì¶œ
      const response = await model.invoke(state.messages);
      return { messages: [response] };
    })
    .addEdge("__start__", "agent")
    .addEdge("agent", "__end__");

  // ê·¸ë˜í”„ ì»´íŒŒì¼
  const app = workflow.compile();

  console.log("ğŸ“ ì˜ˆì œ 1: ê°„ë‹¨í•œ ì§ˆë¬¸\n");

  try {
    const result1 = await app.invoke({
      messages: [
        new HumanMessage("TypeScriptë€ ë¬´ì—‡ì¸ê°€ìš”? ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."),
      ],
    });

    const lastMessage1 = result1.messages[result1.messages.length - 1];
    console.log("ë‹µë³€:", lastMessage1?.content || "ì‘ë‹µ ì—†ìŒ");
  } catch (error) {
    console.error("ì—ëŸ¬ ë°œìƒ:", error);
  }

  console.log("\n" + "=".repeat(50) + "\n");

  console.log("ğŸ“ ì˜ˆì œ 2: ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€\n");

  try {
    const result2 = await app.invoke({
      messages: [
        new HumanMessage("Pythonì´ë€?"),
        new AIMessage("Pythonì€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤."),
        new HumanMessage("ê·¸ëŸ¼ TypeScriptëŠ”?"),
      ],
    });

    const lastMessage2 = result2.messages[result2.messages.length - 1];
    console.log("ë‹µë³€:", lastMessage2?.content || "ì‘ë‹µ ì—†ìŒ");
  } catch (error) {
    console.error("ì—ëŸ¬ ë°œìƒ:", error);
  }

  console.log("\n" + "=".repeat(50) + "\n");

  console.log("ğŸ“ ì˜ˆì œ 3: í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° (model.stream)\n");

  try {
    process.stdout.write("ë‹µë³€: ");

    const stream = await model.stream([
      new HumanMessage("LangGraphì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."),
    ]);

    for await (const chunk of stream) {
      if (typeof chunk.content === "string") {
        process.stdout.write(chunk.content);
      }
    }

    console.log("\n");
  } catch (error) {
    console.error("ì—ëŸ¬ ë°œìƒ:", error);
  }

  console.log("\n" + "=".repeat(50) + "\n");

  console.log("ğŸ“ ì˜ˆì œ 4: LangGraphì—ì„œ ìŠ¤íŠ¸ë¦¬ë°\n");

  try {
    process.stdout.write("ë‹µë³€: ");

    const eventStream = app.streamEvents(
      {
        messages: [new HumanMessage("TypeScriptì˜ ì¥ì ì„ 3ê°€ì§€ë§Œ ë§í•´ì¤˜.")],
      },
      { version: "v2" }
    );

    for await (const event of eventStream) {
      if (event.event === "on_chat_model_stream") {
        const chunk = event.data?.chunk;
        if (chunk?.content && typeof chunk.content === "string") {
          process.stdout.write(chunk.content);
        }
      }
    }

    console.log("\n");
  } catch (error) {
    console.error("ì—ëŸ¬ ë°œìƒ:", error);
  }
}

// ì‹¤í–‰
main().catch(console.error);
